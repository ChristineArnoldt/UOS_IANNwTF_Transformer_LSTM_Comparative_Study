{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from tensorflow.python.ops.ragged import ragged_tensor\n",
    "from tensorflow.python.ops import array_ops_stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddTokenAndPosLayer(tf.keras.layers.Layer):\n",
    "     \n",
    "    def __init__(self, max_input_seq_len, embedding_size):\n",
    "        super(EmbeddTokenAndPosLayer, self).__init__()\n",
    "\n",
    "        self.max_input_seq_len = tf.cast(max_input_seq_len, dtype=tf.float32)\n",
    "        self.embedding_size = tf.cast(embedding_size, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, dtype=tf.float32)\n",
    "        # Generate positional encodings\n",
    "        pos_encoding = self.positional_encoding()\n",
    "\n",
    "        # Expand dimensions to match the input shape\n",
    "        pos_encoding = tf.expand_dims(pos_encoding, axis=0)\n",
    "\n",
    "        # Repeat positional encodings for each example in the batch\n",
    "        pos_encoding = tf.repeat(pos_encoding, tf.shape(x)[0], axis=0)\n",
    "\n",
    "        # Add positional encodings to the input\n",
    "        return x + pos_encoding\n",
    "\n",
    "    def positional_encoding(self):\n",
    "        # Calculate positional encodings\n",
    "        position = tf.range(self.max_input_seq_len, dtype=tf.float32)\n",
    "        position = tf.expand_dims(position, 1)\n",
    "        position = tf.cast(position, dtype=tf.float32)\n",
    "        div_term = tf.pow(tf.cast(10000, dtype=tf.float32), 2 * tf.range(self.embedding_size // 2, dtype=tf.float32) / self.embedding_size)\n",
    "        sinusoid_term = tf.sin(position / div_term)\n",
    "\n",
    "        # Create positional encodings\n",
    "        positional_encoding = tf.concat([sinusoid_term, tf.cos(position / div_term)], axis=-1)\n",
    "\n",
    "        return positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=embedding_size)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(embedding_size, activation=None)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x):\n",
    "        out1 = self.multi_head_attention(x, x)\n",
    "        out1 = self.dropout1(out1)\n",
    "\n",
    "        in_out = self.layernorm1(out1 + x)\n",
    "\n",
    "        out2 = self.dense1(in_out)\n",
    "        out2 = self.dense2(out2)\n",
    "        out2 = self.dropout2(out2)\n",
    "\n",
    "        out2 = self.layernorm2(out2 + in_out)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPredictor(tf.keras.Model):\n",
    "    def __init__(self, vocabulary_size, embedding_size, max_input_seq_len):\n",
    "        super(TokenPredictor, self).__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_input_seq_len = max_input_seq_len\n",
    "        \n",
    "        self.transformer_layer = TransformerBlock(self.embedding_size)\n",
    "        self.positional_layer = EmbeddTokenAndPosLayer(self.max_input_seq_len, self.embedding_size)\n",
    "        self.pooling_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.output_layer = tf.keras.layers.Dense(vocabulary_size, activation=None)\n",
    "        \n",
    "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\")]\n",
    "        \n",
    "        self.layer_list = [\n",
    "            self.positional_layer,\n",
    "            self.transformer_layer,\n",
    "            self.pooling_layer,\n",
    "            self.output_layer\n",
    "        ]\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_state()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layer_list:\n",
    "            try:\n",
    "                x = layer(x,training)\n",
    "            except:\n",
    "                x = layer(x)\n",
    "       \n",
    "        return x\n",
    "  \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        sequence, label = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.call(sequence, training=True)\n",
    "            target_token = tf.expand_dims(label, -1)\n",
    "            loss = self.compiled_loss(label, output)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        \n",
    "        sequence, label = data\n",
    "        target_token = tf.expand_dims(label, -1)\n",
    "\n",
    "        output = self.call(sequence, training=False)\n",
    "        loss = self.compiled_loss(target_token, output)\n",
    "        \n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sequence_length):\n",
    "    X_train = tf.cast(np.random.randint(0, 10, (1024, sequence_length)), tf.float32)\n",
    "    X_train = tf.expand_dims(X_train, axis=-1)\n",
    "    y_train = tf.reduce_sum(X_train[:, :, 0:2], axis=1)  # Addition der ersten beiden Zahlen in jeder Sequenz\n",
    "    y_train = tf.cast(y_train, tf.float32)\n",
    "\n",
    "    X_val = tf.cast(np.random.randint(0, 10, (512, sequence_length)), tf.float32)\n",
    "    X_val = tf.expand_dims(X_val, axis=-1)\n",
    "    y_val = tf.reduce_sum(X_val[:, :, 0:2], axis=1)  # Addition der ersten beiden Zahlen in jeder Sequenz\n",
    "    y_val = tf.cast(y_val, tf.float32)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training für Sequenzlänge: 5\n",
      "Training für Sequenzlänge: 10\n",
      "Training für Sequenzlänge: 15\n",
      "Training für Sequenzlänge: 20\n",
      "Training für Sequenzlänge: 25\n",
      "Training für Sequenzlänge: 30\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib_inline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m histories[sequence_lengths[\u001b[38;5;241m0\u001b[39m]]  \u001b[38;5;66;03m# Rückgabe der Verlaufshistorie für die erste Sequenzlänge\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Trainieren und Plotten des Verlustverlaufs für Sequenzlängen 3, 10 und 20\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_plot_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[228], line 29\u001b[0m, in \u001b[0;36mtrain_and_plot_loss\u001b[0;34m(sequence_lengths)\u001b[0m\n\u001b[1;32m     26\u001b[0m     histories[seq_length] \u001b[38;5;241m=\u001b[39m history  \u001b[38;5;66;03m# Verlaufshistorie für die aktuelle Sequenzlänge speichern\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Plotten des Verlustverlaufs für jede Sequenzlänge\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_length, history \u001b[38;5;129;01min\u001b[39;00m histories\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     31\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequenzlänge \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Train)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/iannwtf_final_project/lib/python3.12/site-packages/matplotlib/pyplot.py:934\u001b[0m, in \u001b[0;36mfigure\u001b[0;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(allnums) \u001b[38;5;241m==\u001b[39m max_open_warning \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    925\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_open_warning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m figures have been opened. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigures created through the pyplot interface \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `matplotlib.pyplot.close()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[0;32m--> 934\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframeon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFigureClass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFigureClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m fig \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fig_label:\n",
      "File \u001b[0;32m~/mambaforge/envs/iannwtf_final_project/lib/python3.12/site-packages/matplotlib/pyplot.py:464\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_figure_manager\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m     \u001b[43m_warn_if_gui_out_of_main_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mnew_figure_manager(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/iannwtf_final_project/lib/python3.12/site-packages/matplotlib/pyplot.py:441\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[0;34m()\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_warn_if_gui_out_of_main_thread\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     canvas_class \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtype\u001b[39m[FigureCanvasBase], \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mFigureCanvas)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m canvas_class\u001b[38;5;241m.\u001b[39mrequired_interactive_framework:\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(threading, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_native_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;66;03m# This compares native thread ids because even if Python-level\u001b[39;00m\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;66;03m# Thread objects match, the underlying OS thread (which is what\u001b[39;00m\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# really matters) may be different on Python implementations with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# green threads.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/iannwtf_final_project/lib/python3.12/site-packages/matplotlib/pyplot.py:280\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03mEnsure that a backend is selected and return it.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03mThis is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _backend_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# Use rcParams._get(\"backend\") to avoid going through the fallback\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# logic (which will (re)import pyplot and then call switch_backend if\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# we need to resolve the auto sentinel)\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;28mtype\u001b[39m[matplotlib\u001b[38;5;241m.\u001b[39mbackend_bases\u001b[38;5;241m.\u001b[39m_Backend], _backend_mod)\n",
      "File \u001b[0;32m~/mambaforge/envs/iannwtf_final_project/lib/python3.12/site-packages/matplotlib/pyplot.py:342\u001b[0m, in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# have to escape the switch on access logic\u001b[39;00m\n\u001b[1;32m    340\u001b[0m old_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(rcParams, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_module_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m canvas_class \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mFigureCanvas\n\u001b[1;32m    345\u001b[0m required_framework \u001b[38;5;241m=\u001b[39m canvas_class\u001b[38;5;241m.\u001b[39mrequired_interactive_framework\n",
      "File \u001b[0;32m~/mambaforge/envs/iannwtf_final_project/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib_inline'"
     ]
    }
   ],
   "source": [
    "def train_and_plot_loss(sequence_lengths):\n",
    "    vocab_size = 6000\n",
    "    embedding_size = 64\n",
    "    histories = {}  # Ein Dictionary zum Speichern der Verlaufshistorien\n",
    "    \n",
    "    for seq_length in sequence_lengths:\n",
    "        print(f\"Training für Sequenzlänge: {seq_length}\")\n",
    "        \n",
    "        # Erstellen des Datensets für die aktuelle Sequenzlänge\n",
    "        X_train, y_train, X_val, y_val = create_dataset(seq_length)\n",
    "        \n",
    "        # Modell erstellen\n",
    "        model = TokenPredictor(vocab_size, embedding_size, seq_length)\n",
    "        \n",
    "        \n",
    "        # Modell kompilieren\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # Modell trainieren\n",
    "        history = model.fit(X_train, y_train, \n",
    "                            epochs=100, \n",
    "                            batch_size=32, \n",
    "                            validation_data=(X_val, y_val),\n",
    "                            verbose=0)  # Verbosity auf 0 setzen, um den Trainingsfortschritt nicht auszugeben\n",
    "        \n",
    "        histories[seq_length] = history  # Verlaufshistorie für die aktuelle Sequenzlänge speichern\n",
    "    \n",
    "    # Plotten des Verlustverlaufs für jede Sequenzlänge\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for seq_length, history in histories.items():\n",
    "        plt.plot(history.history['loss'], label=f\"Sequenzlänge {seq_length} (Train)\")\n",
    "        # plt.plot(history.history['val_loss'], label=f\"Sequenzlänge {seq_length} (Val)\")\n",
    "\n",
    "    plt.title('Verlustverlauf für verschiedene Sequenzlängen')\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('Verlust')\n",
    "    plt.legend()\n",
    "    plt.ylim(0,300)\n",
    "    plt.show()\n",
    "    \n",
    "    return histories[sequence_lengths[0]]  # Rückgabe der Verlaufshistorie für die erste Sequenzlänge\n",
    "\n",
    "# Trainieren und Plotten des Verlustverlaufs für Sequenzlängen 3, 10 und 20\n",
    "history = train_and_plot_loss(sequence_lengths=[5,10,15,20,25,30])\n",
    "\n",
    "print(history.history['loss'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
